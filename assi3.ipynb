{"cells":[{"cell_type":"markdown","metadata":{"id":"CmdlARcjzuVi"},"source":["### Assignment 2\n","Welcome to Assignment 2. This will be fun. It is the first time you actually access external data from ApacheSpark. \n","\n","#### You can also submit partial solutions\n","\n","Just make sure you hit the play button on each cell from top to down. There are three functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"LrlhKpjczuZE"},"source":["This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n","\n","In case you are facing issues, please read the following two documents first:\n","\n","https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n","\n","https://github.com/IBM/skillsnetwork/wiki/FAQ\n","\n","Then, please feel free to ask:\n","\n","https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n","\n","Please make sure to follow the guidelines before asking a question:\n","\n","https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n","\n","\n","If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"K0wpeNFQzuZL","executionInfo":{"status":"ok","timestamp":1652368041207,"user_tz":-330,"elapsed":17,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["from IPython.display import Markdown, display\n","def printmd(string):\n","    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n","\n","\n","if ('sc' in locals() or 'sc' in globals()):\n","    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1m5_Xu3zuZT","executionInfo":{"status":"ok","timestamp":1652368095082,"user_tz":-330,"elapsed":49121,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}},"outputId":"717d8065-ee8a-47e4-bfc6-70e5ac46dd8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark==2.4.5\n","  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n","\u001b[K     |████████████████████████████████| 217.8 MB 6.2 kB/s \n","\u001b[?25hCollecting py4j==0.10.7\n","  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n","\u001b[K     |████████████████████████████████| 197 kB 20.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=3353db1fe2c51506ae63dd61085f186cb311a48e47313ef65eaf89a9bd6c2b0f\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.5\n"]}],"source":["!pip install pyspark==2.4.5"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YDC1vJqvzuZX","executionInfo":{"status":"ok","timestamp":1652368099263,"user_tz":-330,"elapsed":16,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["try:\n","    from pyspark import SparkContext, SparkConf\n","    from pyspark.sql import SparkSession\n","except ImportError as e:\n","    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zEfStaEAzuZb","executionInfo":{"status":"ok","timestamp":1652368112045,"user_tz":-330,"elapsed":8860,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"YZPieEAUzuZg"},"source":["This is the first function you have to implement. You are passed a dataframe object. We've also registered the dataframe in the ApacheSparkSQL catalog - so you can also issue queries against the \"washing\" table using \"spark.sql()\". Hint: To get an idea about the contents of the catalog you can use: spark.catalog.listTables().\n","So now it's time to implement your first function. You are free to use the dataframe API, SQL or RDD API. In case you want to use the RDD API just obtain the encapsulated RDD using \"df.rdd\". You can test the function by running one of the three last cells of this notebook, but please make sure you run the cells from top to down since some are dependant of each other..."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"AzaO3viOzuZl","executionInfo":{"status":"ok","timestamp":1652368286676,"user_tz":-330,"elapsed":473,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["#Please implement a function returning the number of rows in the dataframe\n","def count(df,spark):\n","  df=spark.sql(\"SELECT * from washing\")\n","  return df.count()\n","    #TODO Please enter your code here, you are not required to use the template code below\n","    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n","    #some more help: https://www.w3schools.com/sql/sql_count_avg_sum.asp\n","  return spark.sql('select ### as cnt from washing').first().cnt"]},{"cell_type":"markdown","metadata":{"id":"v_YJpFONzuZq"},"source":["Now it's time to implement the second function. Please return an integer containing the number of fields (columns). The most easy way to get this is using the dataframe API. Hint: You might find the dataframe API documentation useful: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"dnzFSuNczuZv","executionInfo":{"status":"ok","timestamp":1652368377637,"user_tz":-330,"elapsed":461,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["def getNumberOfFields():\n","    #TODO Please enter your code here, you are not required to use the template code below\n","    #df=spark.sql(\"SELECT count(*) \")\n","    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n","    return len(df.columns)"]},{"cell_type":"markdown","metadata":{"id":"0bRxYZ16zuZz"},"source":["Finally, please implement a function which returns a (python) list of string values of the field names in this data frame. Hint: Just copy&past doesn't work because the auto-grader will create a random data frame for testing, so please use the data frame API as well. Again, this is the link to the documentation: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_1mjEtV7zuZ1","executionInfo":{"status":"ok","timestamp":1652368417001,"user_tz":-330,"elapsed":442,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}}},"outputs":[],"source":["def getFieldNames(df,spark):\n","    #TODO Please enter your code here, you are not required to use the template code below\n","    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n","    return df.columns"]},{"cell_type":"markdown","metadata":{"id":"nE5-AS6SzuZ4"},"source":["Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_oJZ-MHzuZ7","executionInfo":{"status":"ok","timestamp":1652368440416,"user_tz":-330,"elapsed":934,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}},"outputId":"a78eada8-7a92-4aab-b119-81b85e279267"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-12 15:13:56--  https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n","Resolving github.com (github.com)... 192.30.255.113\n","Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://github.com/IBM/claimed/blob/master/coursera_ds/washing.parquet?raw=true [following]\n","--2022-05-12 15:13:56--  https://github.com/IBM/claimed/blob/master/coursera_ds/washing.parquet?raw=true\n","Reusing existing connection to github.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github.com/IBM/claimed/raw/master/coursera_ds/washing.parquet [following]\n","--2022-05-12 15:13:56--  https://github.com/IBM/claimed/raw/master/coursera_ds/washing.parquet\n","Reusing existing connection to github.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/IBM/claimed/master/coursera_ds/washing.parquet [following]\n","--2022-05-12 15:13:56--  https://raw.githubusercontent.com/IBM/claimed/master/coursera_ds/washing.parquet\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 112048 (109K) [application/octet-stream]\n","Saving to: ‘washing.parquet?raw=true’\n","\n","washing.parquet?raw 100%[===================>] 109.42K  --.-KB/s    in 0.01s   \n","\n","2022-05-12 15:13:57 (9.45 MB/s) - ‘washing.parquet?raw=true’ saved [112048/112048]\n","\n"]}],"source":["!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n","!mv washing.parquet?raw=true washing.parquet"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VBy-pP5AzuZ-","executionInfo":{"status":"error","timestamp":1652368457674,"user_tz":-330,"elapsed":2329,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}},"outputId":"a665bff6-c5ab-490c-b926-ca0bb539a10e"},"outputs":[{"output_type":"error","ename":"IllegalArgumentException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.parquet.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:633)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-7178ca56ec0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'washing.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'washing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIllegalArgumentException\u001b[0m: 'Unsupported class file major version 55'"]}],"source":["df = spark.read.parquet('washing.parquet')\n","df.createOrReplaceTempView('washing')\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"76N3BGuKzuaA"},"source":["The following cell can be used to test your count function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBGssQsUzuaD"},"outputs":[],"source":["cnt = None\n","nof = None\n","fn = None\n","\n","cnt = count()\n","print(cnt)"]},{"cell_type":"markdown","metadata":{"id":"iHjDeNiFzuaE"},"source":["The following cell can be used to test your getNumberOfFields function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFlIaVyVzuaF"},"outputs":[],"source":["nof = getNumberOfFields()\n","print(nof)"]},{"cell_type":"markdown","metadata":{"id":"ncpSk5NizuaG"},"source":["The following cell can be used to test your getFieldNames function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KVXUl6azuaH"},"outputs":[],"source":["fn = getFieldNames()\n","print(fn)"]},{"cell_type":"markdown","metadata":{"id":"wSOKEqLFzuaI"},"source":["Congratulations, you are done. So please submit your solutions to the grader now.\n","\n","# Start of Assignment-Submission\n","\n","The first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17QTd4NCzuaI","executionInfo":{"status":"ok","timestamp":1652368636381,"user_tz":-330,"elapsed":485,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}},"outputId":"48a03d46-314b-42c1-b863-be3b2c7857e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-12 15:17:12--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2540 (2.5K) [text/plain]\n","Saving to: ‘rklib.py’\n","\n","\rrklib.py              0%[                    ]       0  --.-KB/s               \rrklib.py            100%[===================>]   2.48K  --.-KB/s    in 0s      \n","\n","2022-05-12 15:17:13 (26.1 MB/s) - ‘rklib.py’ saved [2540/2540]\n","\n"]}],"source":["!rm -f rklib.py\n","!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"]},{"cell_type":"markdown","metadata":{"id":"_rXmfKxtzuaL"},"source":["Now it’s time to submit first solution. Please make sure that the token variable contains a valid submission token. You can obtain it from the coursera web page of the course using the grader section of this assignment.\n","\n","Please specify you email address you are using with cousera as well.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"id":"PrjgoPCmzuaN","executionInfo":{"status":"error","timestamp":1652368715350,"user_tz":-330,"elapsed":462,"user":{"displayName":"Niranjan Nagabhushan","userId":"10092624072777997359"}},"outputId":"ac308d94-f338-4a3f-95ea-fadfcc7ef8bf"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-81e7503414ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SVDiVSHNEeiDqw70MIp2vA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please make sure that \"cnt\" is a number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cnt' is not defined"]}],"source":["from rklib import submit, submitAll\n","import json\n","\n","key = \"SVDiVSHNEeiDqw70MIp2vA\"\n","\n","if type(23) != type(cnt):\n","    raise ValueError('Please make sure that \"cnt\" is a number')\n","    \n","if type(23) != type(nof):\n","    raise ValueError('Please make sure that \"nof\" is a number')\n","\n","if type([]) != type(fn):\n","    raise ValueError('Please make sure that \"fn\" is a list')\n","\n","email = \"niranjannagabhushan13@gmail.com\"\n","token = \"NREwdJmtwn75eiyR\"#### your code here ### (have a look here if you need more information on how to obtain the token https://youtu.be/GcDo0Rwe06U?t=276)\n","\n","parts_data = {}\n","parts_data[\"2FjQw\"] = json.dumps(cnt)\n","parts_data[\"j8gMs\"] = json.dumps(nof)\n","parts_data[\"xaauC\"] = json.dumps(fn)\n","\n","\n","submitAll(email, token, key, parts_data)"]},{"cell_type":"code","source":[""],"metadata":{"id":"LYWLnaJS2IqA"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.6","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"assi3.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}